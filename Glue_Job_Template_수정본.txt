import sys
import boto3
import json
from awsglue.gluetypes import *
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
import pg8000
from time import sleep

# --------------------------------------------
# -- @params: [JOB_NAME]
# --------------------------------------------
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# --------------------------------------------
# -- 전역변수
# --------------------------------------------
logger = glueContext.get_logger()
source_db_schema = 'CDCSMART'
source_table_name = ''
target_db_schema = 'CDCSMART'
target_table_name = ''

# --------------------------------------------
# -- 사용자 정의 SQL
# --------------------------------------------
def getSql():
    use_sql = """
        (
        SELECT
		############################
		###########컬럼 나열##########
		############################
        CURRENT_TIMESTAMP AS UPLOAD_DT
        FROM {source_db_schema}.{source_table_name}
        )
        """.strip('\n').replace('\n', ' ').format(source_db_schema=source_db_schema, source_table_name=source_table_name)

    logger.info('use_sql : ' + use_sql)
    return use_sql

# --------------------------------------------
# -- Context 연결
# --------------------------------------------
client = boto3.client('secretsmanager')
response = client.get_secret_value(
    SecretId='kyowon-oracle'
)

secretDict = json.loads(response['SecretString'])

db_username=secretDict['db_username']
db_password=secretDict['db_password']
db_url=secretDict['db_url']
jdbc_driver_name=secretDict['jdbc_driver_name']

bucket_name = 'kyowon-s3-bucket'
sub_folder_name = '/' + target_db_schema + '/'
s3_output_full = "s3://" + bucket_name + sub_folder_name + target_table_name
logger.info('s3 output folder path : ' + s3_output_full)
# --------------------------------------------
# -- Connecting to the source
# --------------------------------------------
df = glueContext.read.format("jdbc").option("driver", jdbc_driver_name).option("url", db_url).option("dbtable", getSql()).option("user", db_username).option("password", db_password).load()

# --------------------------------------------
# -- datasource 설정 
# --------------------------------------------
datasource0 = DynamicFrame.fromDF(df, glueContext, "datasource0")

# --------------------------------------------
# -- Defining mapping for the transformation
# --------------------------------------------
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [
############################
######컬럼 맵핑 정보 기입란######
############################
("UPLOAD_DT","TIMESTAMP","upload_dt","TIMESTAMP")
],
	transformation_ctx = "applymapping1")
resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = "make_struct", transformation_ctx = "resolvechoice2")
dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = "dropnullfields3")

# --------------------------------------------
# -- Deleting to destination
# --------------------------------------------
s3 = boto3.resource('s3')
bucketFull = s3.Bucket(bucket_name)
for obj in bucketFull.objects.filter(Prefix=sub_folder_name[1:] + target_table_name):
    s3.Object(bucketFull.name,obj.key).delete()
    logger.info('deleted s3 output folder : ' + obj.key)
    
# --------------------------------------------
# -- Writing to destination(S3)
# --------------------------------------------
DataSink0 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = "s3", connection_options = {"path": s3_output_full}, format = "parquet", transformation_ctx = "DataSink0")


# --------------------------------------------
# -- Redshift Connection & Copy data to red
# --------------------------------------------
try :
    red_response = client.get_secret_value(
    SecretId='kywon-redshift'
    )
    red_secretDict = json.loads(red_response['SecretString'])
    database=red_secretDict['database']
    host=red_secretDict['host']
    port=red_secretDict['port']
    user=red_secretDict['user'] 
    password=red_secretDict['password'] 
    
    conn = pg8000.connect(host=host,port=port,database=database,user=user,password=password)
    cursor = conn.cursor()
    try : 
        cursor.execute("truncate {target_db_schema}.{target_table_name}".format(target_db_schema=target_db_schema,target_table_name=target_table_name))
        sleep(5)        
        cursor.execute("copy {target_db_schema}.{target_table_name} from 's3://kyowon-s3-bucket/{target_db_schema}/{target_table_name}' iam_role 'arn:aws:iam::014262325615:role/kyowon-rs-role' format as parquet".format(target_db_schema=target_db_schema,target_table_name=target_table_name))
    except Exception as e:
        print(e)
        exit()
    conn.commit()
except Exception as e:
    print(e)

job.commit()
